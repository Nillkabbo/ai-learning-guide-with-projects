# Chapter 2: Tokens, Embeddings, and Context Windows - Cursor Rules

## Chapter Context

This chapter covers the three fundamental "physics" of AI: tokens (cost), embeddings (meaning), and context windows (memory). These concepts are essential for building efficient, cost-effective AI applications.

## Key Concepts

### 1. Tokens: The Currency
- Tokens are chunks of text (words, parts of words, punctuation)
- Different models use different tokenizers
- Cost = tokens (input + output) × price per token
- Token count ≠ word count (usually ~0.75 tokens per word)
- Concise prompts save money
- Use tiktoken to count tokens: `encoder.encode(text)`

### 2. Embeddings: Semantic Understanding
- Embeddings = text converted to vectors (lists of numbers)
- Similar meanings = similar vectors (close in vector space)
- Can compare embeddings using cosine similarity
- Different models produce different dimensions:
  - OpenAI text-embedding-3-small: 1536 dimensions
  - Ollama nomic-embed-text: 768 dimensions
  - Ollama all-minilm: 384 dimensions
- Cannot directly compare embeddings from different models
- Semantic search uses embeddings to find meaning-based matches

### 3. Context Windows: Limited Memory
- Context window = maximum tokens AI can consider
- Everything counts: system + user + assistant messages
- Exceeding limit = AI forgets earliest messages
- Popular model limits:
  - gpt-4o-mini: 128,000 tokens
  - claude-3-5-sonnet: 200,000 tokens
  - gemini-1.5-pro: 1,000,000 tokens
- Must manage context for long conversations
- Sliding window: keep system message, remove oldest messages

## Important Code Patterns

### Token Counting
```python
import tiktoken
encoder = tiktoken.encoding_for_model("gpt-4o-mini")
tokens = encoder.encode(text)
token_count = len(tokens)
```

### Cost Estimation
```python
COST_PER_1M_INPUT_TOKENS = 0.15  # gpt-4o-mini example
COST_PER_1M_OUTPUT_TOKENS = 0.60
input_cost = (input_tokens / 1_000_000) * COST_PER_1M_INPUT_TOKENS
output_cost = (output_tokens / 1_000_000) * COST_PER_1M_OUTPUT_TOKENS
total_cost = input_cost + output_cost
```

### Creating Embeddings (OpenAI)
```python
import openai
client = openai.OpenAI()
response = client.embeddings.create(
    model="text-embedding-3-small",
    input=text  # Can be string or list of strings
)
embedding = response.data[0].embedding  # List of 1536 floats
```

### Creating Embeddings (Ollama)
```python
import ollama
response = ollama.embeddings(
    model="nomic-embed-text",
    prompt=text
)
embedding = response["embedding"]  # List of 768 floats
```

### Cosine Similarity
```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """Calculate cosine similarity between two vectors."""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    # Normalize
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return np.dot(vec1, vec2) / (norm1 * norm2)
```

### Context Management
```python
import tiktoken

def manage_context(messages: list[dict], max_tokens: int = 120000) -> list[dict]:
    """Manage conversation to stay within token limit."""
    encoder = tiktoken.encoding_for_model("gpt-4o-mini")
    
    # Always keep system message
    system_msg = None
    if messages and messages[0]['role'] == 'system':
        system_msg = messages.pop(0)
    
    # Calculate tokens and trim if needed
    current_tokens = sum(len(encoder.encode(msg['content'])) for msg in messages)
    while current_tokens > max_tokens and messages:
        removed = messages.pop(0)
        current_tokens -= len(encoder.encode(removed['content']))
    
    # Restore system message
    if system_msg:
        messages.insert(0, system_msg)
    
    return messages
```

## Common Mistakes to Avoid

1. **Token Counting Errors**
   - Using wrong encoder for model
   - Forgetting to count output tokens
   - Not accounting for system messages

2. **Embedding Mistakes**
   - Comparing embeddings from different models
   - Not normalizing vectors before similarity
   - Recreating embeddings unnecessarily (cache them!)

3. **Context Management Issues**
   - Removing system message
   - Not tracking token usage
   - Removing important context too early
   - Forgetting that everything counts toward limit

4. **Cost Estimation Errors**
   - Only counting input tokens
   - Using wrong pricing
   - Not accounting for different model prices

## Integration Points

### Prerequisites (Chapter 1)
- Understanding of message structure
- Basic AI API calls
- System/user/assistant roles

### Next Chapter Preparation (Chapter 3)
- Context management will be used in development environment
- Token counting helps with cost optimization
- Embeddings prepare for more advanced applications

## Project Context

### Chapter 2 Project: Smart IoT Troubleshooting Assistant
- **Type**: Cumulative (builds on Chapter 1)
- **Focus**: Token counting, embeddings, semantic search, context management
- **Key Skills**: Cost optimization, semantic understanding, memory management
- **Extension Opportunities**: Caching, multiple knowledge bases, visualization

## Code Style and Best Practices

### Token Counting
- Always use the correct encoder for your model
- Count both input and output tokens
- Display token usage to users (transparency)
- Cache token counts when possible

### Embeddings
- Cache embeddings for knowledge bases (don't recreate)
- Normalize vectors before comparison
- Use appropriate embedding model for your use case
- Handle batch operations efficiently (OpenAI supports batching)

### Context Management
- Always preserve system message
- Remove oldest messages first (FIFO)
- Consider summarizing instead of removing
- Monitor token usage in production
- Set reasonable max_tokens (leave buffer, don't use 100%)

## Learning Progression

### What Students Should Master
1. ✅ Can count tokens accurately
2. ✅ Can estimate API costs
3. ✅ Understands what embeddings represent
4. ✅ Can create and compare embeddings
5. ✅ Can implement semantic search
6. ✅ Understands context window limits
7. ✅ Can manage conversation context

### Common Student Questions
- "Why are tokens different from words?" → Tokenization is optimized for models
- "Do I need to understand vector math?" → No, just understand the concept
- "Can I mix embedding models?" → No, stick to one model per application
- "What happens if I exceed context?" → API errors or truncation—manage proactively
- "How do I choose an embedding model?" → Balance of cost, speed, and dimensions

## Technical Notes

### Tokenization Details
- Common words: often 1 token
- Rare words: often multiple tokens
- Punctuation: usually separate tokens
- Numbers: often split into digits
- Different models = different tokenization

### Embedding Dimensions
- Higher dimensions = more nuanced understanding
- But also = more storage and computation
- 768-1536 dimensions is typical sweet spot
- Lower dimensions (384) are faster but less nuanced

### Context Window Strategies
- **Sliding Window**: Remove oldest messages
- **Summarization**: Summarize old messages instead of removing
- **Hierarchical**: Keep important messages, remove less important
- **Chunking**: Split long documents into chunks

## Related Chapters

- **Chapter 1**: Message structure (used in context management)
- **Chapter 3**: Development environment (uses context management)
- **Chapter 8**: API patterns (extends token/cost optimization)
- **Chapter 20**: Scaling (uses embeddings and context management)
- **Chapter 27**: RAG (heavily uses embeddings and context)

## Memory Context for Next Session

When continuing to Chapter 3, remember:
- Students understand token counting and cost
- They know how embeddings work
- They can implement semantic search
- They understand context window limits
- They can manage conversation context
- These concepts will be used throughout the book

## Project-Specific Context

The Smart IoT Troubleshooting Assistant:
- Combines all three Chapter 2 concepts
- Demonstrates practical application
- Shows cost optimization in action
- Uses semantic search for real problem-solving
- Manages context for multi-turn conversations
- Should be completable in 4-5 hours for beginners

## Advanced Notes

### Token Optimization
- Use concise prompts
- Remove unnecessary words
- Structure prompts efficiently
- Consider prompt templates

### Embedding Optimization
- Cache embeddings (don't recreate)
- Batch operations when possible
- Use appropriate model size
- Consider dimensionality reduction if needed

### Context Optimization
- Summarize instead of removing
- Prioritize important messages
- Use external storage for old context
- Implement hierarchical context management

