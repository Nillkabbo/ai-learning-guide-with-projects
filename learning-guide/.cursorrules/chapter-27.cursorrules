# Chapter 27: Retrieval-Augmented Generation (RAG) - Cursor Rules

## Chapter Context

This chapter covers RAG (Retrieval-Augmented Generation) - the most important technique for knowledge-intensive AI applications. Students learn document indexing, semantic search, vector databases, and building complete RAG systems. Essential for applications that need up-to-date or private knowledge.

## Key Concepts

### 1. The Problem
- LLMs have static knowledge (frozen at training time)
- No access to private documents
- No recent information
- Can't answer questions about specific entities
- RAG solves this by providing context

### 2. RAG Solution
- "Open-book exam" approach
- Two-phase workflow:
  - **Indexing (offline)**: Process documents → chunks → embeddings → vector DB
  - **Retrieval (real-time)**: Query → embedding → search → retrieve chunks → augment prompt → generate

### 3. Indexing Phase
- Document loading (PDF, TXT, DOCX, etc.)
- Chunking (200-500 tokens, 50-100 token overlap)
- Embedding generation (convert chunks to vectors)
- Vector database storage (ChromaDB, Pinecone, Weaviate)
- Index optimization

### 4. Retrieval Phase
- Query embedding (convert question to vector)
- Semantic search (find similar chunks)
- Top-k retrieval (get most relevant chunks)
- Context augmentation (add chunks to prompt)
- Prompt construction (system + context + query)

### 5. Complete RAG System
- End-to-end workflow
- Quality optimization (chunking, retrieval strategies)
- Evaluation (answer quality, retrieval accuracy)
- Error handling (no results, low confidence)

## Important Code Patterns

### Indexing
```python
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import chromadb

# Load documents
loader = PyPDFLoader("document.pdf")
documents = loader.load()

# Chunk documents
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(documents)

# Generate embeddings and store
chroma_client = chromadb.Client()
collection = chroma_client.create_collection("documents")

for i, chunk in enumerate(chunks):
    embedding = get_embedding(chunk.page_content)
    collection.add(
        ids=[str(i)],
        embeddings=[embedding],
        documents=[chunk.page_content]
    )
```

### Retrieval
```python
def rag_query(question: str):
    # Embed query
    query_embedding = get_embedding(question)
    
    # Search vector database
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=3
    )
    
    # Retrieve relevant chunks
    context = "\n\n".join(results['documents'][0])
    
    # Augment prompt
    prompt = f"""
    Use the following context to answer the question.
    
    Context:
    {context}
    
    Question: {question}
    """
    
    # Generate answer
    response = ai.chat(prompt)
    return response
```

## Common Mistakes to Avoid

1. **Chunks too large**: Exceeds context window
2. **Chunks too small**: Lose context
3. **No overlap**: Lose continuity
4. **Wrong retrieval count**: Too few = missing info, too many = noise
5. **No source citation**: Can't verify answers

## Integration Points

- **Chapter 2**: Embeddings (foundation for RAG)
- **Chapter 26**: Fine-tuning (alternative to RAG)
- **Chapter 28**: Workflows (RAG in workflows)

## Related Chapters

- **Chapter 2**: Embeddings (foundation)
- **Chapter 26**: Fine-tuning (alternative)
- **Chapter 28**: Workflows (RAG in workflows)
