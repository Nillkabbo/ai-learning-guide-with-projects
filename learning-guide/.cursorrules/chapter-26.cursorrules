# Chapter 26: Fine-Tuning and Custom Models - Cursor Rules

## Chapter Context

This chapter covers fine-tuning custom AI models on your data. Students learn when to fine-tune vs. prompt engineering vs. RAG, data preparation, fine-tuning workflow, model evaluation, and deployment. Advanced technique for specialized AI systems.

## Key Concepts

### 1. When to Fine-Tune
- **Fine-tuning is for skills, not facts**
- Use for: specific style, complex format, niche domain, new skill
- Don't use for: factual recall (use RAG), simple style (use prompts), rapidly changing info (use RAG)
- Decision framework: Prompt → RAG → Fine-tuning

### 2. Data Preparation (80% of Work)
- Quality over quantity (50-100 high-quality examples often enough)
- JSONL format (one JSON object per line)
- Message structure: system, user, assistant
- Consistency in format and style
- Clarity in examples

### 3. Fine-Tuning Workflow
- Upload training data to API
- Create fine-tuning job
- Monitor progress
- Wait for completion
- Retrieve fine-tuned model ID
- Use model like base model

### 4. Model Evaluation
- Compare to base model
- Test on validation set
- Measure improvements
- Quality assessment
- Cost-benefit analysis

### 5. Deployment
- Use fine-tuned model ID
- API integration
- Version management
- Cost considerations (fine-tuned models cost more)
- Monitoring

## Important Code Patterns

### Data Preparation
```python
import json

def create_finetuning_dataset(examples: list[dict], output_file: str):
    """Creates a .jsonl file for fine-tuning."""
    with open(output_file, 'w') as f:
        for example in examples:
            f.write(json.dumps(example) + '\n')

# Example format
example = {
    "messages": [
        {"role": "system", "content": "You are an IoT log classifier."},
        {"role": "user", "content": "Log: Device failed to connect."},
        {"role": "assistant", "content": "Connectivity"}
    ]
}
```

### Fine-Tuning (OpenAI)
```python
from openai import OpenAI

client = OpenAI()

# Upload training file
with open("training_data.jsonl", "rb") as f:
    training_file = client.files.create(file=f, purpose="fine-tune")

# Create fine-tuning job
job = client.fine_tuning.jobs.create(
    training_file=training_file.id,
    model="gpt-3.5-turbo"
)

# Monitor progress
status = client.fine_tuning.jobs.retrieve(job.id)
print(f"Status: {status.status}")

# Use fine-tuned model
response = client.chat.completions.create(
    model=status.fine_tuned_model,  # Use fine-tuned model ID
    messages=[{"role": "user", "content": "Classify this log..."}]
)
```

## Common Mistakes to Avoid

1. **Fine-tuning for facts**: Use RAG instead
2. **Low-quality data**: Garbage in, garbage out
3. **Too much data**: Quality over quantity
4. **No evaluation**: Must compare to base model
5. **Ignoring costs**: Fine-tuned models cost more per token

## Integration Points

- **Chapter 9-12**: Prompt engineering (alternative to fine-tuning)
- **Chapter 27**: RAG (alternative to fine-tuning)
- **Chapter 25**: Deployment (deploy fine-tuned models)

## Related Chapters

- **Chapters 9-12**: Prompt engineering (alternative)
- **Chapter 27**: RAG (alternative)
- **Chapter 25**: Deployment (deploy models)
