# Chapter 21: AI Application Cost Optimization - Cursor Rules

## Chapter Context

This chapter teaches cost optimization strategies for AI applications. Students learn intelligent caching, batch processing, model selection, cost monitoring, and ROI calculation. Can reduce costs by 50-90%.

## Key Concepts

### 1. Cost Structure
- Token-based pricing
- Input tokens (cheaper)
- Output tokens (more expensive)
- Model price differences (10-100x)
- Hidden costs (retries, context)

### 2. Intelligent Caching
- Response caching (exact matches)
- Semantic caching (similar prompts)
- Prompt-based caching
- Cache invalidation
- 50-80% cost reduction possible

### 3. Batch Processing
- Process multiple items together
- Maximize token value
- Reduce API calls
- Cost efficiency
- 30-50% savings

### 4. Model Selection
- Right model for task
- Cost/quality tradeoff
- Model routing
- Automatic selection
- 10-100x cost differences

### 5. Cost Monitoring
- Real-time tracking
- Dashboards
- Alerts
- Budget management
- ROI calculation

## Important Code Patterns

### Cost Calculator
```python
class CostCalculator:
    PRICING = {
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-4o": {"input": 5.00, "output": 15.00}
    }
    
    def calculate(self, model, input_tokens, output_tokens):
        pricing = self.PRICING[model]
        input_cost = (input_tokens / 1_000_000) * pricing["input"]
        output_cost = (output_tokens / 1_000_000) * pricing["output"]
        return input_cost + output_cost
```

### Semantic Caching
```python
def semantic_cache_key(prompt):
    # Create embedding
    embedding = get_embedding(prompt)
    # Find similar cached prompts
    similar = find_similar_embeddings(embedding, threshold=0.95)
    if similar:
        return similar[0].cache_key
    return None
```

### Batch Processing
```python
def process_batch(items, batch_size=10):
    batches = [items[i:i+batch_size] for i in range(0, len(items), batch_size)]
    results = []
    for batch in batches:
        # Process batch together
        result = ai.process_batch(batch)
        results.extend(result)
    return results
```

## Common Mistakes to Avoid

1. **No caching**: Wastes money on repeated requests
2. **Wrong model**: Using expensive model for simple tasks
3. **No batching**: Processing items individually
4. **No monitoring**: Can't optimize what you don't measure
5. **Ignoring context growth**: Context accumulates, costs grow

## Integration Points

- **Chapter 2**: Tokens (cost basis)
- **Chapter 8**: API patterns (apply optimization)
- **Chapter 20**: Scaling (costs scale with usage)

## Related Chapters

- **Chapter 2**: Tokens (cost basis)
- **Chapter 8**: API patterns (apply optimization)
- **Chapter 20**: Scaling (costs scale)
