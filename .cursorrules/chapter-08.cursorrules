# Chapter 8: API Design Patterns and Best Practices - Cursor Rules

## Chapter Context

This chapter covers production-grade patterns for robust AI systems. Students learn error handling, retries, rate limiting, caching, streaming decisions, and multi-provider architectures. Essential for moving from prototypes to production.

## Key Concepts

### 1. Robust Error Handling
- Catch specific exception types
- Different handling per error type
- RateLimitError: retryable
- AuthenticationError: not retryable
- Graceful degradation

### 2. Retries with Exponential Backoff
- Exponential backoff: 1s, 2s, 4s, 8s...
- Jitter: random variation to prevent thundering herd
- Max retries: prevent infinite loops
- Retryable vs. non-retryable errors

### 3. Rate Limiting
- Token bucket algorithm
- Sliding window
- Per-user limits
- Provider-specific quotas
- Rate limit monitoring

### 4. Caching Strategies
- Response caching (same prompt = cached)
- Prompt-based caching
- Multi-level caching (memory, Redis)
- Cache invalidation
- Cost and latency reduction

### 5. Streaming vs. Batching
- Streaming: real-time UX, long responses
- Batching: cost efficiency, throughput
- Adaptive strategies
- Hybrid approaches

### 6. Multi-Provider Architecture
- Provider abstraction layer
- Load balancing
- Automatic failover
- Health checks
- High availability

## Important Code Patterns

### Error Handling
```python
try:
    response = client.chat.completions.create(...)
except openai.RateLimitError:
    # Retry with backoff
except openai.AuthenticationError:
    # Don't retry, fix auth
except openai.APIError:
    # Log and handle
```

### Exponential Backoff
```python
import time
import random

def retry_with_backoff(func, max_retries=3):
    for attempt in range(max_retries):
        try:
            return func()
        except RetryableError:
            if attempt == max_retries - 1:
                raise
            wait = (2 ** attempt) + random.uniform(0, 1)
            time.sleep(wait)
```

### Rate Limiting
```python
from collections import deque
import time

class TokenBucket:
    def __init__(self, capacity, refill_rate):
        self.capacity = capacity
        self.tokens = capacity
        self.refill_rate = refill_rate
        self.last_refill = time.time()
    
    def consume(self, tokens=1):
        self._refill()
        if self.tokens >= tokens:
            self.tokens -= tokens
            return True
        return False
```

### Caching
```python
import hashlib
import json
from functools import lru_cache

def cache_key(prompt, model):
    return hashlib.md5(f"{prompt}:{model}".encode()).hexdigest()

@lru_cache(maxsize=1000)
def cached_ai_call(prompt, model):
    # Check cache first
    # If miss, call API and cache
    pass
```

## Common Mistakes to Avoid

1. **Generic error handling**: Always catch specific exceptions
2. **Immediate retries**: Use exponential backoff
3. **No rate limiting**: Will hit provider limits
4. **No caching**: Wastes money and time
5. **Wrong streaming choice**: Choose based on use case
6. **Single provider**: No redundancy for production

## Integration Points

- **Chapters 5-7**: Apply patterns to all APIs
- **Chapter 20**: Scaling extends these patterns
- **Chapter 23**: Monitoring complements patterns

## Related Chapters

- **Chapters 5-7**: API basics (apply patterns)
- **Chapter 20**: Scaling (extends patterns)
- **Chapter 23**: Monitoring (complements patterns)
